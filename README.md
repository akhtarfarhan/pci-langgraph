# PCI LangGraph Chatbot ğŸ§©ğŸ”®  

A tiny, fully-local demo that stitches together **LangGraph**, **FastAPI**, **Streamlit**, and a **local LLM served by [Ollama](https://ollama.com)**.  
Ask any product-related question; the graph will (1) detect the customer segment, (2) call an LLM, then (3) show a three-line suggestion in the UI.

---

## âœ¨ Project Highlights
| Layer | Library | File | Purpose |
|-------|---------|------|---------|
| **Frontend** | Streamlit | `frontend/app.py` | Simple chat UI â†” backend HTTP |
| **API** | FastAPI | `backend/main.py` | `/query` & `/health` endpoints |
| **State machine** | LangGraph | `backend/graph.py` | context â†’ segment â†’ suggestion |
| **LLM call** | Ollama Python | `backend/suggestion.py` | local inference (no cloud costs) |
| **Memory / checkpoints** | SQLite | `backend/memory.py` | conversation-level persistence |

---

## ğŸ–¥ï¸ Quick Start (TL;DR)

```bash
git clone https://github.com/akhtarfarhan/pci_langgraph.git
cd pci_langgraph

# create & activate virtual-env
python -m venv .venv
.\.venv\Scripts\ctivate      # PowerShell / cmd
# source .venv/bin/activate   # Bash / zsh

pip install -r requirements.txt

# 1ï¸âƒ£ make sure one Ollama daemon is running
ollama serve                    # leave it in its own terminal
ollama pull llama3:8b           # once, ~4 GB

# 2ï¸âƒ£ backend API
uvicorn backend.main:app --host 0.0.0.0 --port 8000 --reload

# 3ï¸âƒ£ frontend UI
streamlit run frontend/app.py
```

Open **http://localhost:8501** â†’ chat away!

---

## ğŸ—‚ï¸ Repo Structure

```
pci_langgraph/
â”‚
â”œâ”€ backend/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ main.py          # FastAPI app
â”‚  â”œâ”€ graph.py         # LangGraph pipeline
â”‚  â”œâ”€ segmentation.py  # heuristic segment detector
â”‚  â”œâ”€ suggestion.py    # Ollama prompt + call
â”‚  â””â”€ memory.py        # SQLite checkpoint helper
â”‚
â”œâ”€ frontend/
â”‚  â””â”€ app.py           # Streamlit chat UI
â”‚
â”œâ”€ data/               # created at runtime for SQLite DB
â”œâ”€ .gitignore
â”œâ”€ requirements.txt
â””â”€ README.md
```

---

## ğŸ”Œ Detailed Setup

### 1. Prerequisites
| Tool | Version | Notes |
|------|---------|-------|
| Python | 3.9 â€“ 3.12 | Tested on 3.11 |
| Git | any | for cloning / pushing |
| Ollama | 0.1.32 + | <https://ollama.com/download> |
| (Windows) VC++ Redistributable | latest | Ollama requirement |

> **GPU?** Not requiredâ€”CPU runs fine; GPU (CUDA/Metal) speeds things up.

### 2. Clone & virtual-env

```bash
git clone https://github.com/akhtarfarhan/pci_langgraph.git
cd pci_langgraph
python -m venv .venv
.\.venv\Scriptsctivate          # Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Start the LLM

```bash
ollama serve                 # one instance only
ollama pull llama3:8b        # change model in backend/suggestion.py if desired
```

### 5. Run the backend API

```bash
uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000
# -> http://localhost:8000/docs  (FastAPI Swagger UI)
```

### 6. Run the Streamlit frontend

```bash
streamlit run frontend/app.py
# -> http://localhost:8501
```

---

## ğŸ› ï¸ Configuration

| Variable / file | Default | Meaning |
|-----------------|---------|---------|
| `MODEL_NAME` in `backend/suggestion.py` | `"llama3:8b"` | Any model you have locally (e.g. `llama2:13b`, `phi3:mini`) |
| `BACKEND` in `frontend/app.py` | `http://localhost:8000/query` | Change if you host the API elsewhere |
| `.venv/` | *ignored* | Python virtual-env |
| `data/pci_memory.sqlite` | runtime | Conversation checkpoints |

---

## ğŸ” API Reference

### `GET /health`
Simple liveness probe.

```bash
curl http://localhost:8000/health
# {"status":"ok"}
```

### `POST /query`
```json
Request body:
{
  "messages": [
    {"role": "user", "content": "I want a cheap phone"}
  ]
}
Headers:
session-id: demo
```

```json
Successful response:
{
  "segment": "Value-Seeker",
  "suggestion": "Looks like youâ€™re after an affordable option â€¦",
  "messages": [
    {"role": "user", "content": "I want a cheap phone"},
    {"role": "assistant", "content": "Looks like youâ€™re after â€¦"}
  ]
}
```

---

## ğŸš‘ Troubleshooting

| Symptom | Fix |
|---------|-----|
| `Error: listen tcp 127.0.0.1:11434` | Another Ollama instance already runningâ€”use it or kill it. |
| `AttributeError: module 'ollama' has no attribute 'ollama'` | Youâ€™re on the new client API; ensure `backend/suggestion.py` matches *this* repo. |
| Backend crashes with `SqliteSaver` arg error | You overwrote `backend/memory.py`â€”use the version here or upgrade `langgraph`. |
| Streamlit shows `Backend error 422: missing "session-id"` | `headers={"session-id": â€¦}` (hyphen) in `frontend/app.py`. |

---

## ğŸ§¹ .gitignore (excerpt)

```gitignore
__pycache__/
*.py[cod]
.venv/
.vscode/
data/
*.sqlite
.DS_Store
Thumbs.db
```

---

## ğŸ‘©â€ğŸ’» Development Tips

* **Hot-reload**: backend runs with `--reload`; Streamlit reloads on save.
* **Branching**: `git checkout -b feature/<name>` â†’ push â†’ open PR.
* **Linting/formatting**: add `ruff`, `black`, or `isort` as you like.
* **Testing**: integrate `pytest`; mock Ollama calls via `responses`.

---

## ğŸ“œ License

MIT Â© 2025 Farhan Akhtar  
See [`LICENSE`](LICENSE) for details.

---

## ğŸ™ Credits

* [LangGraph](https://github.com/langchain-ai/langgraph) â€“ stateful graphs for LLMs  
* [FastAPI](https://fastapi.tiangolo.com/) â€“ blazing-fast web framework  
* [Streamlit](https://streamlit.io/) â€“ the fastest way to build data apps  
* [Ollama](https://ollama.com) â€“ run LLMs locally with one command  
* Initial integration and troubleshooting compiled with help from ChatGPT.
* 

#PROJECT REPORT
# PCIÂ LangGraphÂ ChatbotÂ â€” ProjectÂ Report

---

## 1Â Â Overview

**PCIÂ LangGraphÂ Chatbot** is a fullyâ€‘local proofâ€‘ofâ€‘concept that stitches together four modern Python stacks:

| Layer         | Library                   | Role                                                          |
| ------------- | ------------------------- | ------------------------------------------------------------- |
| UI            | **Streamlit**             | Chat interface in the browser                                 |
| API           | **FastAPI** + **Uvicorn** | REST endpoint (`/query`) that LangGraph invokes               |
| State Machine | **LangGraph**             | 3â€‘step graph â†’ â‘  context â‘¡ segment detection â‘¢ LLM suggestion |
| LLM Runtime   | **Ollama**                | Local LLM (default: `llama3:8b`) for zero cloud cost          |

The application detects a userâ€™s customer segment (Valueâ€‘Seeker, Premium, Techâ€‘Enthusiast, General), calls a local LLM to craft a short product suggestion, and streams the result back to the UI.

---

## 2Â Â Architecture Diagram

```mermaid
flowchart LR
    subgraph Frontend
        A[StreamlitÂ UI]
    end
    subgraph Backend
        B(FastAPIÂ / Uvicorn)
        C[LangGraphÂ Graph\ncontextÂ â†’Â segmentÂ â†’Â suggest]
        D[CheckpointÂ Store\n(SQLiteÂ orÂ Memory)]
        E[OllamaÂ Daemon\n(localÂ LLM)]
    end

    A -- HTTP â†’ B
    B -- invoke --> C
    C -- load/dump --> D
    C -- prompt âœ E
    E -- response --> C
    C -- JSON --> B
    B -- JSON --> A
```

---

## 3Â Â Module Breakdown

| File | Responsibility                                                     |
| ---- | ------------------------------------------------------------------ |
| ``   | Streamlit chat UI, talks to backend via `requests`                 |
| ``   | FastAPI app (`/health`, `/query`)                                  |
| ``   | Defines `StateGraph` with three nodes                              |
| ``   | Heuristic customerâ€‘segment classifier                              |
| ``   | Builds prompt & calls Ollama â€‘> returns suggestion                 |
| ``   | Provides bestâ€‘available checkpointer (SQLite âš™ï¸â†’ Memory ğŸ§ â†’ Noâ€‘op) |
| ``   | Pin runtime dependencies                                           |
| ``   | Excludes venv, data, logs, build artefacts                         |

---

## 4Â Â Setup &Â Execution

```bash
# clone & enter
git clone https://github.com/akhtarfarhan/pci_langgraph.git
cd pci_langgraph

# venv
python -m venv .venv
.\.venv\Scripts\activate   # (or source .venv/bin/activate)

# deps
pip install -r requirements.txt

# oneâ€‘time model pull
ollama serve        # separate terminal
ullama pull llama3:8b

# backend
.\.venv\Scripts\python.exe -m uvicorn backend.main:app --reload

# frontend (new terminal)
streamlit run frontend/app.py
```

Open [**http://localhost:8501**](http://localhost:8501) and chat away.

---

## 5Â Â KeyÂ DesignÂ Decisions

1. **Localâ€‘first**: All inference runs onâ€‘device via Ollama â†’ no API keys, works offline.
2. **Resilient checkpointing**: `memory.py` falls back gracefully if SQLite extras are missing.
3. **Stateless API, stateful graph**: FastAPI handles each request statelessly; LangGraph manages perâ€‘session memory via checkpoint store.
4. **Hotâ€‘reload everywhere**: Uvicornâ€™s `--reload` + Streamlitâ€™s autoâ€‘refresh speeds up dev loop.

---

## 6Â Â Reflections &Â Challenges

| Challenge                                                  | Solution / Lesson                                                                      |
| ---------------------------------------------------------- | -------------------------------------------------------------------------------------- |
| Dependency drift in LangGraph (`SqliteSaver` path changes) | Added multiâ€‘path import + memory fallback to keep code futureâ€‘proof.                   |
| Windows fileâ€‘lock on `.venv` DLLs during Git rebase        | `.gitignore` excludes `.venv/`; rebasing issues vanished.                              |
| Conflicting Python interpreters (Anaconda vs venv)         | Always launch with venvâ€™s explicit `python.exe`.                                       |
| Streamlit 422 error (missing `sessionâ€‘id` header)          | Changed header name to hyphen version to satisfy FastAPIâ€™s underscoreâ€‘conversion rule. |

Overall, the project demonstrates how small, focused Python modules can be composed into a full localâ€‘LLM product demo with minimal lines of code.

---

## 7Â Â FutureÂ Work

- **Persistent user profiles** (e.g. Redis backend instead of SQLite).
- **Rich UI**: product carousels, images, and ratings via Streamlit components.
- **Better segmentation**: train a small ML model or use embedding similarity.
- **Docker container** for pushâ€‘button deployment.

---

## 8Â Â DemoÂ Checklist

-

---

> **Author:** FarhanÂ Akhtar â€” JulyÂ 2025


